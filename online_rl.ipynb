{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0ee8c7",
   "metadata": {},
   "source": [
    "### Online Reforce Learning(RL) using Group Relative Policy Optimization (GRPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import libraries\n",
    "import torch\n",
    "from transformers import TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import  GRPOTrainer, GRPOConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba3509",
   "metadata": {},
   "source": [
    "##### Helpers function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, full_message=None,  max_new_tokens=500):\n",
    "    # Format chat using tokenizer's chat template\n",
    "  \n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        full_message, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "\n",
    "    # Convert messages to token IDs, send to device incase the model is on gpu\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    # Recommended to use vllm, sgland or TensorRt\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    # extract the generated ids and the responses\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    # Decode response to text base response\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_with_questions(model, tokenizer, questions,\n",
    "                             system_message=None, title=\"Model Output\"):\n",
    "    print(f\"\\n==== {title} ====\\n\")\n",
    "    rows = []\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        response = generate_responses(model, tokenizer, question, system_message)\n",
    "        rows.append({\"User Prompt\": question, \"Assistant Response\": response})\n",
    "    df = pd.DataFrame(rows)\n",
    "    pd.set_option('display.max_colwidth', None) # avoid truncating long text\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb208df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, use_gpu=True):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from the model name\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    if use_gpu:\n",
    "        model.to(\"cuda\")\n",
    "    # if the model doesn't have a chat template, we need to define it\n",
    "    if not tokenizer.chat_template:\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "            {% if message['role'] == 'system' % }System: {{ message['content'] }}\\n\n",
    "            {% elif message['role'] == 'user' % }User: {{ message['content'] }}\\n\n",
    "            {% elif message['role'] == 'assistant' % }Assistant: {{ message['content'] }} <|endoftext|>\\n\n",
    "            {% endif %}\n",
    "            {% endfor %}\"\"\"\n",
    "    # Tokenizer config\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b10892",
   "metadata": {},
   "source": [
    "#### Prepare for evaluation dataset for Math: GSM8K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are helpful assistant that solves problems step-by-step.\"\n",
    "    \"Always include the final numeric answer inside \\\\boxed{}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(completions, ground_truth, **kwargs):\n",
    "    # Regular expression to capture content inside \\boxed{}\n",
    "    matches = [re.search(r\"\\\\boxed\\{(.*?)\\}\", completion[0]['content']) for completion in completions]\n",
    "    contents = [match.group(1) if match else \"\" for match in matches]\n",
    "    # Reward 1 if the content is the same as the ground truth, 0 otherwise\n",
    "    rewards = [1.0 if c == gt else 0.0 for c, gt in zip(contents, ground_truth)]\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": r\"... Calculating the answer. \\boxed{72}\"\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "ground_truth = [\"72\"]\n",
    "reward = reward_func(sample_pred, ground_truth)\n",
    "print(f\"Positive Sample Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2055a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": r\"... Calculating the answer. \\boxed{71}\"\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "ground_truth = [\"72\"]\n",
    "reward = reward_func(sample_pred, ground_truth)\n",
    "print(f\"Negative Sample Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eb7acf",
   "metadata": {},
   "source": [
    "#### Load the Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef50826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the display configures in pandas\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', 0)\n",
    "\n",
    "data_num = 5\n",
    "eval_dataset = load_dataset(\"openai/gsm8k\", \"main\")[\"test\"].select(range(data_num))\n",
    "sample_df = eval_dataset.to_pandas()\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(example):\n",
    "    match = re.search(r\"####\\s*(-?\\d+)\", example[\"answer\"])\n",
    "    example[\"ground_truth\"] = match.group(1) if match else None\n",
    "    example[\"prompt\"] = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": example[\"question\"]\n",
    "        }\n",
    "    ]\n",
    "    return example\n",
    "eval_dataset = eval_dataset.map(post_processing).remove_columns([\"question\", \"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f89a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = eval_dataset.select(range(5)).to_pandas()\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc2eb7f",
   "metadata": {},
   "source": [
    "### Load the model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f06fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"Qwen/Qwen2.5-0.5B-Instruct\", USE_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Store predictions and ground truths\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for example in tqdm(eval_dataset):\n",
    "    input_prompt = example[\"prompt\"]\n",
    "    ground_truth = example[\"ground_truth\"]\n",
    "    #2.Run the model to generate an answer\n",
    "    with torch.no_grad():\n",
    "        response = generate_responses(model, tokenizer, full_message=input_prompt)\n",
    "    all_preds.append([\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response\n",
    "        }\n",
    "    ])\n",
    "    all_labels.append(ground_truth)\n",
    " #  print(f\"Predicted: {response}\")\n",
    " #  print(f\"Ground Truth: {ground_truth}\")\n",
    "\n",
    "#3. Evaluate using reward_func\n",
    "rewards = reward_func(all_preds, all_labels)\n",
    "\n",
    "#4. Report accuracy\n",
    "accuracy = sum(rewards) / len(rewards)\n",
    "#rint(f\"Evaluation Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "df = pd.DataFrame({\"predictions\": all_preds, \"ground_truth\": all_labels, \"rewards\": rewards})\n",
    "display(df)\n",
    "print(f\"Evaluation Accuracy: {accuracy:.2%}\")\n",
    "del model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b26c2",
   "metadata": {},
   "source": [
    "### Loading the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74853608",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Apply to dataset\n",
    "train_dataset = train_dataset.map(post_processing)\n",
    "train_dataset = train_dataset.remove_columns([\"question\", \"answer\"])\n",
    "if not USE_GPU:\n",
    "    train_dataset = train_dataset.select(range(20))\n",
    "# limit the dataset to 100 examples for testing on gpu for now\n",
    "train_dataset = train_dataset.select(range(30))\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0649dc",
   "metadata": {},
   "source": [
    "#### GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff0ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GRPOConfig(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_generations=4, # Can set as high as 64 or 128\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    logging_steps=2,\n",
    "    no_cuda=USE_GPU, # Set to False if you want to use GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b10898",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if this block hangs or the kernel restarts during training, please skip loading the previous 0.5B model for evaluation\n",
    "model, tokenizer = load_model_and_tokenizer(\"Qwen/Qwen2.5-0.5B-Instruct\", USE_GPU)\n",
    "\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    args=config,\n",
    "    reward_funcs=reward_func,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "grpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48179935",
   "metadata": {},
   "source": [
    "### Results of the fully trained Qwen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6da0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grpo_trainer.model\n",
    "\n",
    "# Store predictions and ground truths\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for example in tqdm(eval_dataset):\n",
    "    input_prompt = example[\"prompt\"]\n",
    "    ground_truth = example[\"ground_truth\"]\n",
    "    #2.Run the model to generate an answer\n",
    "    with torch.no_grad():\n",
    "        response = generate_responses(model, tokenizer, full_message=input_prompt)\n",
    "    all_preds.append([\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response\n",
    "        }\n",
    "    ])\n",
    "    all_labels.append(ground_truth)\n",
    "# 3. Evaluate using reward_func\n",
    "rewards = reward_func(all_preds, all_labels)\n",
    "# 4. Report accuracy\n",
    "accuracy = sum(rewards) / len(rewards)\n",
    "\n",
    "df = pd.DataFrame({\"predictions\": all_preds, \"ground_truth\": all_labels, \"rewards\": rewards})\n",
    "display(df)\n",
    "print(f\"Evaluation Accuracy: {accuracy:.2%}\")\n",
    "del model, tokenizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbb7b5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
